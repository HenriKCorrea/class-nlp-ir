{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiLOq51Z9gHU"
      },
      "source": [
        "**CMP617**\n",
        "\n",
        "\n",
        "\n",
        "Objetivo: Gerar um classificador de polaridade utilizando word embeddings e LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX6EBvN2O2RS"
      },
      "source": [
        "#Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (1.3.8)\n",
            "Requirement already satisfied: nltk in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (3.10.1)\n",
            "Requirement already satisfied: seaborn in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (1.6.1)\n",
            "Requirement already satisfied: yellowbrick in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (1.5)\n",
            "Requirement already satisfied: setuptools in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (65.5.1)\n",
            "Requirement already satisfied: plotly in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (6.0.1)\n",
            "Requirement already satisfied: nbformat in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (5.10.4)\n",
            "Requirement already satisfied: tensorflow in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (2.19.0)\n",
            "Requirement already satisfied: gensim==4.3.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (4.3.1)\n",
            "Requirement already satisfied: scipy==1.10.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from gensim==4.3.1) (7.1.0)\n",
            "Requirement already satisfied: click in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from plotly) (1.33.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nbformat) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from nbformat) (5.14.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n",
            "Requirement already satisfied: rich in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /workspaces/class-nlp-ir/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install environment dependencies\n",
        "%pip install unidecode nltk pandas numpy matplotlib seaborn scikit-learn yellowbrick setuptools plotly nbformat tensorflow gensim==4.3.1 scipy==1.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QbNeBVcgtyFC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMhsfGp0v9Q0"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzls6277tPMZ"
      },
      "source": [
        "## AmericanasBR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FlM6h2tJiAr",
        "outputId": "f77ab484-30b8-42db-d89c-629cceb9bfa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  981k  100  981k    0     0   705k      0  0:00:01  0:00:01 --:--:--  706k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1657k  100 1657k    0     0   887k      0  0:00:01  0:00:01 --:--:--  887k\n"
          ]
        }
      ],
      "source": [
        "#baixando os datasets\n",
        "!curl https://www.inf.ufrgs.br/~viviane/DS/B2W-Reviews01_binario_TEST.csv > B2W-Reviews01_binario_TEST.csv\n",
        "!curl https://www.inf.ufrgs.br/~viviane/DS/B2W-Reviews01_binario5000_TRAIN.csv > B2W-Reviews01_binario5000_TRAIN.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jSVPHBniLnjS"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('B2W-Reviews01_binario5000_TRAIN.csv')\n",
        "df_test = pd.read_csv('B2W-Reviews01_binario_TEST.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yjZ8ogxsFAi"
      },
      "source": [
        "# Classificação com embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o9_WxCdapH1R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-04 14:09:31.338475: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-04 14:09:31.344721: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-04 14:09:31.355057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743775771.369940   13262 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743775771.374316   13262 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1743775771.387207   13262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1743775771.387218   13262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1743775771.387220   13262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1743775771.387222   13262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-04 14:09:31.391604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import TextVectorization\n",
        "from keras.layers import Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pfFiPVQhHwNR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epocas\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tW88oEgb6u0"
      },
      "source": [
        "## Pré-processamento\n",
        "\n",
        "Atividade opcional: verificar resultado com script de pré-processamento do [NILC](http://www.nilc.icmc.usp.br/embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3VxFjeTXc00p"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcBcy7QsW-x5",
        "outputId": "1de8c860-d5e3-45e1-9ede-984ecf43bb3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pandas.core.common import random_state\n",
        "#função de pré-processamento\n",
        "special_chars = \"¨'!#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "# stop_words = stopwords.words('portuguese')\n",
        "def preprocess(x):\n",
        "    new_x = x.replace('\"',' ')\n",
        "    for c in special_chars:\n",
        "        new_x = new_x.replace(c,' ')\n",
        "    # new_x = ' '.join([word for word in nltk.word_tokenize(new_x.lower(),language='portuguese') if word not in stop_words]) #removendo stop words\n",
        "    new_x = re.sub(r'[^\\w\\s]', ' ', new_x) #removendo pontuação do texto\n",
        "    new_x = re.sub(\"http\\S+\", ' ',new_x) # remove links\n",
        "    new_x = re.sub(\"@\\w+\", ' ',new_x) # remove contas com @\n",
        "    new_x = re.sub('#\\S+', ' ',new_x) # hashtags\n",
        "    new_x = re.sub('[0-9]+', ' ',new_x) # remove numeros e palavras com numeros\n",
        "    #new_x = unidecode(new_x) #acentos\n",
        "    new_x = re.sub(\"\\s+\", ' ',new_x) # espaços\n",
        "    new_x = new_x.strip()\n",
        "    new_x = new_x.lower()\n",
        "    return new_x\n",
        "\n",
        "#pré-processar datasets de treino e teste\n",
        "df_train['text_original'] = df_train['text']\n",
        "df_train['text'] = df_train['text'].apply(preprocess)\n",
        "\n",
        "df_test['text_original'] = df_test['text']\n",
        "df_test['text'] = df_test['text'].apply(preprocess)\n",
        "\n",
        "df_train = df_train.sample(n=len(df_train), random_state=42).copy()\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "len(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfIU5zykNU3w",
        "outputId": "86165116-81fa-47e3-b60d-4298eb796e97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9996"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove instâncias com texto com comprimento zero ou nulo\n",
        "df_train = df_train[df_train['text']!='']\n",
        "df_train = df_train[~df_train['text'].isna()]\n",
        "df_train.reset_index(drop=True, inplace=True) # reindexa dataframe\n",
        "len(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at0DPjKFgwEv",
        "outputId": "0f409a06-04af-4e50-8d5d-2fd4c7b49477"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9926"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove duplicidades\n",
        "dup = df_train[df_train.duplicated(subset=['text'])]\n",
        "df_train = df_train.drop(dup.index)\n",
        "df_train.reset_index(drop=True, inplace=True) # reindexa dataframe\n",
        "len(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PE56gma08mC"
      },
      "source": [
        "## Preparando os datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccejxiLIfYCN",
        "outputId": "59dd6fa4-eb3b-4574-bb22-99eeedef23f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9926, 6000)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_train), len(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FXk-XuI_fdBu",
        "outputId": "22392292-4ad1-4b16-d917-900489277fac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_descr</th>\n",
              "      <th>text_original</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6645</th>\n",
              "      <td>1</td>\n",
              "      <td>produto muito bom valor justo supriu a necessi...</td>\n",
              "      <td>positivo</td>\n",
              "      <td>Produto muito bom. Valor justo, supriu a neces...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3024</th>\n",
              "      <td>1</td>\n",
              "      <td>gostei da escova muito pratica de usar e chego...</td>\n",
              "      <td>positivo</td>\n",
              "      <td>Gostei da escova ,muito pratica de usar e cheg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5855</th>\n",
              "      <td>1</td>\n",
              "      <td>possui cheiro forte mas o importante é que rea...</td>\n",
              "      <td>positivo</td>\n",
              "      <td>Possui cheiro forte, mas o importante é que re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8968</th>\n",
              "      <td>0</td>\n",
              "      <td>não era o produto q eu esperava pois na compra...</td>\n",
              "      <td>negativo</td>\n",
              "      <td>Não era o produto q eu esperava pois na compra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5910</th>\n",
              "      <td>0</td>\n",
              "      <td>como posso avaliar um produto que ainda não ch...</td>\n",
              "      <td>negativo</td>\n",
              "      <td>como posso avaliar um produto que ainda não ch...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text label_descr  \\\n",
              "6645      1  produto muito bom valor justo supriu a necessi...    positivo   \n",
              "3024      1  gostei da escova muito pratica de usar e chego...    positivo   \n",
              "5855      1  possui cheiro forte mas o importante é que rea...    positivo   \n",
              "8968      0  não era o produto q eu esperava pois na compra...    negativo   \n",
              "5910      0  como posso avaliar um produto que ainda não ch...    negativo   \n",
              "\n",
              "                                          text_original  \n",
              "6645  Produto muito bom. Valor justo, supriu a neces...  \n",
              "3024  Gostei da escova ,muito pratica de usar e cheg...  \n",
              "5855  Possui cheiro forte, mas o importante é que re...  \n",
              "8968  Não era o produto q eu esperava pois na compra...  \n",
              "5910  como posso avaliar um produto que ainda não ch...  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.sample(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6L0m5RBXkl"
      },
      "source": [
        "Separa treino em treino e validação. O conjunto de validação é ser usado durante treinamento da rede."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBavJoFYj1fb",
        "outputId": "a654c49f-757f-41d0-e3c1-4692021ac7a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((7940,), (1986,), (6000,))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split treino em: treino e val\n",
        "train_samples, val_samples, train_labels, val_labels = train_test_split(\n",
        "    df_train['text'].values, df_train['label'].values, test_size=0.2, random_state=42) #reprodutibilidade dos datasets gerados\n",
        "\n",
        "test_samples = df_test['text'].values\n",
        "test_labels = df_test['label'].values\n",
        "\n",
        "train_samples.shape, val_samples.shape, test_samples.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n0TYFSUkRw0",
        "outputId": "b6b5f35e-e567-4c35-ca49-b247ef115d96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('péssimo pedi começo do ano e ainda tá aguardando nota fiscal com previsão de entrega só em abril como uma compradora regular do site estou insatisfeita',\n",
              " 'lamentavelmente na descrição do produto não informa que precisa utilizar o cabo usb e o vga sendo assim quando comprei o monitor tive que fazer um outro investimento do conversor de vga para hdmi pois meu notebook não tem entrada vga a americanas deveria informar no descritivo do produto essa questão pois o cliente não tem ciência do problema informado acima')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_samples[10], val_samples[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pZaeOpplS6m"
      },
      "source": [
        "## Criando o codificador para o vocabulário\n",
        "\n",
        "O `vectorizer` é instanciado usando o `TextVectorization` do Keras que fará a indexação do vocabulario do dataset.\n",
        "\n",
        "Ou seja, `vectorizer` fará a codificação das instâncias. Aqui o vocabulário considera 20.000 palavras e fará o truncamento ou padding das instâncias para que todas tenham 128 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uJVUokDIlS6n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-04 14:09:34.294616: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGHT = 128\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=20000, #vocabulário maximo\n",
        "    output_sequence_length=MAX_LENGHT,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwgrnFMzVmgo",
        "outputId": "279d20d0-cdad-43fe-cbac-e7846f371cf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, ['', '[UNK]'])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# antes de adapatar vectorizer a um conjunto de textos o cvocabulario so tem 2 tokens:\n",
        "len(vectorizer.get_vocabulary()), vectorizer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2oS-nj8PHHQ",
        "outputId": "debed25b-251a-4c5a-9d96-fdc4a970c7d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12471"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# adapapta:\n",
        "vectorizer.adapt(train_samples)\n",
        "len(vectorizer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaL18X8wlS6n",
        "outputId": "a1f85229-c38a-47ae-87c4-aa28555dffb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulário tem 12471 tokens. Os primeiros 10 tokens são:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'o',\n",
              " 'e',\n",
              " 'a',\n",
              " 'de',\n",
              " 'não',\n",
              " 'produto',\n",
              " 'que',\n",
              " 'muito',\n",
              " 'é',\n",
              " 'com',\n",
              " 'do',\n",
              " 'um',\n",
              " 'para',\n",
              " 'da',\n",
              " 'em',\n",
              " 'no',\n",
              " 'entrega',\n",
              " 'recomendo',\n",
              " 'mais',\n",
              " 'uma',\n",
              " 'na',\n",
              " 'bom',\n",
              " 'mas',\n",
              " 'chegou',\n",
              " 'foi',\n",
              " 'se',\n",
              " 'comprei',\n",
              " 'eu',\n",
              " 'qualidade',\n",
              " 'as',\n",
              " 'americanas',\n",
              " 'prazo',\n",
              " 'recebi',\n",
              " 'ainda',\n",
              " 'bem',\n",
              " 'me',\n",
              " 'já',\n",
              " 'tem',\n",
              " 'por',\n",
              " 'como',\n",
              " 'meu',\n",
              " 'estou',\n",
              " 'excelente',\n",
              " 'até',\n",
              " 'os',\n",
              " 'antes',\n",
              " 'compra',\n",
              " 'veio',\n",
              " 'pra',\n",
              " 'minha',\n",
              " 'super',\n",
              " 'sem',\n",
              " 'só',\n",
              " 'pois',\n",
              " 'ótimo',\n",
              " 'dia',\n",
              " 'gostei',\n",
              " 'nao',\n",
              " 'dias',\n",
              " 'ser',\n",
              " 'nem',\n",
              " 'boa',\n",
              " 'mesmo',\n",
              " 'ao',\n",
              " 'ele',\n",
              " 'nada',\n",
              " 'esse',\n",
              " 'está',\n",
              " 'comprar',\n",
              " 'loja',\n",
              " 'agora',\n",
              " 'site',\n",
              " 'ou',\n",
              " 'tudo',\n",
              " 'aparelho',\n",
              " 'pelo',\n",
              " 'celular',\n",
              " 'ter',\n",
              " 'quando',\n",
              " 'rápida',\n",
              " 'isso',\n",
              " 'tive',\n",
              " 'uso',\n",
              " 'preço',\n",
              " 'rápido',\n",
              " 'entregue',\n",
              " 'tenho',\n",
              " 'melhor',\n",
              " 'problema',\n",
              " 'funciona',\n",
              " 'ótima',\n",
              " 'minhas',\n",
              " 'das',\n",
              " 'era',\n",
              " 'tempo',\n",
              " 'q',\n",
              " 'fácil',\n",
              " 'defeito']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# agora vemos os tokens do dataset começando pelos mais frequentes:\n",
        "voc = vectorizer.get_vocabulary()\n",
        "print(f'Vocabulário tem {len(voc)} tokens. Os primeiros 10 tokens são:')\n",
        "voc[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amIQbu-HlS6n"
      },
      "source": [
        "Vetorizando uma sentença:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fapfWXWLlS6n",
        "outputId": "043d6866-be4b-4157-956f-3eb66cd079bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 128]),\n",
              " <tf.Tensor: shape=(1, 128), dtype=int64, numpy=\n",
              " array([[2267,  691,   22,    1,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0]])>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = vectorizer([[\"chuva forte na floresta\"]])\n",
        "output.shape, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kpuk1y4N-t0",
        "outputId": "3231af4b-992e-4b9e-e6c3-d932aebdce0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('chuva', 'forte', 'na', '[UNK]')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# conferindo as palavras que correnspodem a alguns dos índices gerados na condificação:\n",
        "voc[2267],voc[691], voc[22], voc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwVrcoM_lS6o",
        "outputId": "4461c593-76e1-412c-b079-825a954c87a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12471"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# um dicionario com todas as palavras do vocabulario e seus indices de 0 a len(voc)-1\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "len(voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH4UeadIQpps",
        "outputId": "b2ab9dae-cfe4-45e5-f4f1-85b8cd98b6cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2267"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_index['chuva']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owZsRFiHlS6p"
      },
      "source": [
        "## Usando embeddings pré-treinadas\n",
        "####Há várias opções de embeddings já treinadas em português no site do NILC nesse [link](http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INmx9tQKrocB",
        "outputId": "b9495a60-2491-495f-d6d2-52679a0b9236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  299M  100  299M    0     0  10.6M      0  0:00:28  0:00:28 --:--:-- 10.6M\n",
            "Archive:  skip_s100.zip\n",
            "  inflating: skip_s100.txt           \n"
          ]
        }
      ],
      "source": [
        "#Opção 1: baixando as embeddings do NILC\n",
        "# !curl http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip > glove_s100.zip\n",
        "# !unzip -o glove_s100.zip\n",
        "\n",
        "!curl http://143.107.183.175:22980/download.php?file=embeddings/fasttext/skip_s100.zip > skip_s100.zip\n",
        "!unzip -o skip_s100.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "s2a_VogyM9Ex"
      },
      "outputs": [],
      "source": [
        "#embFile = 'fast_cbow_s50.txt'\n",
        "#embFile = 'fast_skip_s100.txt'\n",
        "embFile = 'skip_s100.txt'\n",
        "#embFile = 'w2v_skip_s300.txt'\n",
        "# embFile = 'glove_s100.txt'\n",
        "embedding_dim = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLkzZsJINL52"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#Opção 2 para quem tem as embeddings no seu drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# embFile = f\"/content/drive/MyDrive/Colab Notebooks/embeddings/{embFile}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76JBEOqElS6p",
        "outputId": "56813436-5970-436c-98f4-5ca57727d477"
      },
      "outputs": [],
      "source": [
        "# verificando o arquivo de embeddings e carregando para uma matriz: cada linha da matriz tem o numero de dimensões das embeddings\n",
        "embeddings_index = {}\n",
        "with open(embFile) as f:\n",
        "    l=0\n",
        "    for line in f:\n",
        "        if l>0:\n",
        "            pos = line.find(' ')\n",
        "            word = line[0:pos]\n",
        "            vector = line[pos:len(line)]\n",
        "            vector = vector.replace('\\n','')\n",
        "            vector = vector.strip()\n",
        "            vector = list(map(float, vector.split(' ')))\n",
        "            embeddings_index[word] = vector\n",
        "        l += 1\n",
        "print(f\"Encontrados {len(embeddings_index)} vetores de palavras.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGMMgjon-ElN"
      },
      "source": [
        "Verificando palavras do arquivo de embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEIgmb9jg95O",
        "outputId": "fdbc34f7-2ae1-47c4-e9b4-90a87b505a9b"
      },
      "outputs": [],
      "source": [
        "primeira = list(embeddings_index.keys())[0]\n",
        "print('primeiro token: ',primeira)\n",
        "print('embeddigns do primeiro token: ',embeddings_index[primeira])\n",
        "print('---------------')\n",
        "centesima = list(embeddings_index.keys())[100]\n",
        "print('centésimo token: ',centesima)\n",
        "print('embeddigns do centesimo token: ',embeddings_index[centesima])\n",
        "print('---------------')\n",
        "ultima = list(embeddings_index.keys())[len(embeddings_index)-1]\n",
        "print('ultimo token: ',ultima)\n",
        "print('embeddigns do ultimo token: ',embeddings_index[ultima])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DAs8uEQpm2p",
        "outputId": "4a869c94-66be-4825-f828-b2366956b13e"
      },
      "outputs": [],
      "source": [
        "len(word_index), len(voc),len(embeddings_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnQiJgvfz0If",
        "outputId": "3b11ea3f-190f-4dd8-a429-2dd14e7875a1"
      },
      "outputs": [],
      "source": [
        "word_index['mais']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elr_h0ZF95fB"
      },
      "source": [
        "O proximo passo é efetuar o mapeamento entre as embeddings e as palavras do vocabulário do dataset. Prepara-se uma matriz de embeddings: uma matriz NumPy simples onde a entrada no índice `i` é o vetor pré-treinado para a palavra de índice `i` no vocabulário do nosso `vetorizador`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E02DsI-c3WmB",
        "outputId": "43629bcd-4f2b-4f02-9d61-8fa3ed03d1ac"
      },
      "outputs": [],
      "source": [
        "embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqd3XL7IlS6q",
        "outputId": "3016a3a0-3f00-4ffa-8954-4a6cfd44d33e"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(voc)\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Preparando  a matriz de embeddings\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in embeddings_index:\n",
        "        # print(word, i, len(embeddings_index[word]))\n",
        "        # Palavras não encontradas no índice de embeddings terão seus valores na matriz zerados\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(f\"Mapeadas {hits} palavras e {misses} não existem nas embeddings (OOV))\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRgITllfojwU",
        "outputId": "65a350ce-357e-4a6c-bcaa-7a77886c3671"
      },
      "outputs": [],
      "source": [
        "# Tamanho da matrix de embeddings e do vocabulario:\n",
        "embedding_matrix.shape, len(vectorizer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTmsH9U8lS6q"
      },
      "source": [
        "## Preparando o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HP8FP_cdPON"
      },
      "source": [
        "### Embedding layer\n",
        "\n",
        "A matriz `embedding_matrix` é usada na definição do layer de embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NudJqEsICqpi",
        "outputId": "25f19848-3fd2-47a6-8b57-fec3c2cedf48"
      },
      "outputs": [],
      "source": [
        "num_tokens, embedding_dim, embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UziYa38F_Pap"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    trainable=False, #mantem o layer de embeddgins com os valores que está e nao treina isso junto coma  rede\n",
        ")\n",
        "embedding_layer.build((1,))\n",
        "#usa os pesos das embeddings\n",
        "embedding_layer.set_weights([embedding_matrix])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iMKqANk5UyR"
      },
      "source": [
        "Atividade opcional: treinar as embeddgins do dataset (por exemplo usando gensim, ou usando a propria rede)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZaRBvbC1rOx"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cFi6Rgg6kW-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "6mtKMOX7_uzq",
        "outputId": "39e1acad-f617-4f6f-f73c-ce52de95264d"
      },
      "outputs": [],
      "source": [
        "#configura um vetor de inputs que tera os identificadores do vocabulario\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# converte as embeddgins de cada input\n",
        "embedded_sequences = embedding_layer(inputs)\n",
        "\n",
        "# adiciona uma LSTM bidirecional\n",
        "x = layers.Bidirectional(layers.LSTM(64))(embedded_sequences)\n",
        "\n",
        "# adiciona um classificador binário\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# Cria o modelo\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "#Exibe o resumo do modelo e suas camadas\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Ib_y1D4cJb"
      },
      "source": [
        "Estamos usando `binary_crossentropy` como loss para a classificação binária.\n",
        "\n",
        "[`Adam`](https://keras.io/api/optimizers/adam/) é o otimizador usado com `learning_rate` default de 0.001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w9tAw5vX4LA"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "x9Ka-P6SnIXm",
        "outputId": "7ad32195-8842-4949-fdc7-5504d0c722c1"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, show_layer_names=False, rankdir='LR', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGcPvQThlS6r"
      },
      "source": [
        "## Treinando o modelo\n",
        "\n",
        "Convertendo o texto dos 3 datasets (treino, val e teste) em vetores com índices do vocabulário, usando o vetorizador definido anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2MBI58VKI9w",
        "outputId": "b6151a1d-4724-47b5-aa6f-1bff3b1176e6"
      },
      "outputs": [],
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "x_test = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "x_train.shape, x_val.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imqG3txONU84",
        "outputId": "65cd91b4-6c91-44ee-b6fb-276e8195e8ad"
      },
      "outputs": [],
      "source": [
        "i=10\n",
        "train_samples[i], x_train[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eLgQCNgyKBV",
        "outputId": "4fd01e4c-3caa-4605-bcea-016e55f5e4bc"
      },
      "outputs": [],
      "source": [
        "class_names = df_train.label_descr.unique()\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfu4OQUBJprt",
        "outputId": "c63f1838-8b45-42a7-b652-6120d43a6f6b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "EzQ3i8LC6BaE",
        "outputId": "dd8a5afb-3826-4b25-8abd-a1d6da980c34"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nprn2BAAA66"
      },
      "source": [
        "## Avaliando a Qualidade do Modelo\n",
        "\n",
        "\n",
        "A função `model.evaluate` prevê a saída para a entrada fornecida e, em seguida, calcula a função de métricas especificadas em `model.compile` com base em `y_true` e `y_pred`.\n",
        "\n",
        "O `model.predict` apenas retorna o y_pred, mas ao final devem chegar no mesmo valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNzOQfeBa4q0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-LVYUpjQa4Nd",
        "outputId": "061e0c6f-d64b-4ff7-a365-e8023506417e"
      },
      "outputs": [],
      "source": [
        "df_test.sample(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAZYIvQ9-yBA",
        "outputId": "f2571a0d-5955-450e-9837-0d32db2b8830"
      },
      "outputs": [],
      "source": [
        "i=10\n",
        "train_samples[i], x_train[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KZP4_shLbet",
        "outputId": "6ea24ccb-bed0-45e5-fdc8-ffe06eae1732"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose = False)\n",
        "print(f'Test Loss: {test_loss:.5f}')\n",
        "print(f'Test Accuracy: {test_acc:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddyjr8taabHx",
        "outputId": "3a5902d6-0123-4135-9d0b-c92dfa899648"
      },
      "outputs": [],
      "source": [
        "y_prob = model.predict(np.array(x_test))\n",
        "# convertendo o vetor de probabilidads em um vetor de predições binárias\n",
        "# y_pred = [int(i > .5) for i in y_prob]\n",
        "y_pred = [int(i > .5) for i in y_prob.flatten()]\n",
        "print(f\"Acurácia: {accuracy_score(y_test, y_pred):.5f}\")\n",
        "y_pred[0:15]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "QDhEqtS0Z5XF",
        "outputId": "2d4d1e65-0f85-455b-d7cd-231926bd09d1"
      },
      "outputs": [],
      "source": [
        "#matriz de confusão\n",
        "print(f\"Acurácia: {accuracy_score(y_test, y_pred):.5f}\")\n",
        "print(f\"F1-macro: {f1_score(y_test, y_pred, average='macro'):.5f}\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['negative','positive'])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANabzDzFX7cA"
      },
      "source": [
        "##Examinando as instâncias mal classificadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgYIwunMQP5f"
      },
      "outputs": [],
      "source": [
        "erros  = list (zip(df_test['text'].values,df_test['text_original'].values,y_test,y_pred)) #criando lista com os erros\n",
        "erros = [item for item in erros if item[2] != item[3]] #removendo as instâncias corretas\n",
        "df_erros = pd.DataFrame(erros,columns =['Texto','Original','True','Pred']) #gerando um dataframe para ficar mais fácil de trabalhar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q6gwLXhQivh"
      },
      "outputs": [],
      "source": [
        "#acrescentando colunas FP e FN no dataframe com os erros\n",
        "df_erros['FP'] = df_erros.apply(lambda x: 1 if ((x['Pred']==1) & (x['True']==0)) else 0, axis=1)\n",
        "df_erros['FN'] = df_erros.apply(lambda x: 1 if ((x['Pred']==0) & (x['True']==1)) else 0, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDUISBJw9J1i",
        "outputId": "7eb1f5db-30c0-492c-fe48-625462039d66"
      },
      "outputs": [],
      "source": [
        "print('Há ', len(df_erros),' instâncias mal classificadas.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "NdIdgqUIjSXN",
        "outputId": "a8b4acc6-265e-43e3-d9b2-e4ada9579810"
      },
      "outputs": [],
      "source": [
        "#inspecionando os falsos negativos\n",
        "df_erros[df_erros['FN']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pMdQqblpYEXz",
        "outputId": "ac6955b6-13d8-478c-852c-4b49fc1b66ef"
      },
      "outputs": [],
      "source": [
        "#inspecionando os falsos positivos\n",
        "df_erros[df_erros['FP']==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgZr0mEcBevY"
      },
      "source": [
        "# Persistência\n",
        "\n",
        "Para persistencia do modelo e utilizar o mesmo sem precisar retreinar:\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/save_and_serialize"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
